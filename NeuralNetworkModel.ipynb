from io import BytesIO

import boto3 as b3
import pandas as pds
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Flatten, Input
from tensorflow.keras.optimizers import Adam

# Initial set-up: --------------------------------------------------------------------------------

# Grab s3
s3 = b3.client('s3')

# Bucket set-up
the_bucket = 'groupproject-balducci'
books_key = 'path/to/books_data.csv'
ratings_key = 'path/to/books_rating.csv'

# Fetch books data and store
books_data_object = s3.get_object(Bucket = the_bucket, Key = books_key)
books_data = pds.read_csv(BytesIO(books_data_object['Body'].read()))

# Fetch user ratings data and store
books_rating_object = s3.get_object(Bucket = the_bucket, Key = ratings_key)
books_rating = pds.read_csv(BytesIO(books_rating_object['Body'].read()))

# Merge data together (Merge with 'Title' as the key and only merge rows that occur in both files)
all_data = pds.merge(books_data, books_rating, on = 'Title', how = 'inner')

# Data Preprocessing: ----------------------------------------------------------------------------

# Remove any rows with NaN values
all_data = all_data.dropna()

# Create a matrix of users (rows) to books (columns) based on user ratings
rating_matrix = all_data.pivot_table(index = 'User_id', columns = 'Title', values = 'review/score')

# If user hasn't rated a book add a zero
rating_matrix = rating_matrix.fillna(0)

# Neural Network Model: --------------------------------------------------------------------------

# Grab counts:
user_count = len(rating_matrix.index)
item_count = len(rating_matrix.columns)

# Feature count per user (Smaller is less costly but less accurate)
embedding_size = 10

# Model start
model = Sequential()

# Using User_id as the input (1)
model.add(Input(shape = [1]))

# Convert each user to into a profile of embedding_size
model.add(Embedding(input_dim = user_count, output_dim = embedding_size, input_length = 1))

# Flatten profiles into single lines
model.add(Flatten())

# Create the hidden layer (64 neurons)
model.add(Dense(64, activation = 'relu'))

# Create the output layer (sigmoid for generating probability)
model.add(Dense(item_count, activation = 'sigmoid'))

# Compile time! (Using Regression)
model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['mae'])